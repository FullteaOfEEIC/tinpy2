{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import subprocess\n",
    "import MeCab\n",
    "import glob\n",
    "import xgboost as xgb\n",
    "import cv2\n",
    "from multiprocessing import Pool\n",
    "import copy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.linear_model import SGDRegressor, HuberRegressor, TheilSenRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, ReLU, Input, GlobalAveragePooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']\n",
    "\n",
    "cmd = 'echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"'\n",
    "path = (subprocess.Popen(cmd, stdout=subprocess.PIPE,\n",
    "                           shell=True).communicate()[0]).decode('utf-8')\n",
    "m = MeCab.Tagger(\"-d {0}\".format(path))\n",
    "m_wakati = MeCab.Tagger(\"-d {0} -Owakati\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"data/tinder.xlsx\"\n",
    "imagePath = \"data/photos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(filePath)\n",
    "df.drop_duplicates(inplace=True, subset=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c1f2aeb9ce42c89cbfc4411d5cc9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10590.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"bio\"] = df[\"bio\"].fillna(\"\")\n",
    "\n",
    "def getPhotoNum(_id):\n",
    "    return len(glob.glob(os.path.join(imagePath,\"{0}-*.jpg\".format(_id))))\n",
    "\n",
    "with Pool() as p:\n",
    "    imap=p.imap(getPhotoNum, df[\"id\"])\n",
    "    df[\"photo_num\"]=np.asarray(list(tqdm(imap, total=df[\"id\"].shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, train_size=0.8, random_state=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios=[]\n",
    "for bio in df_train.bio:\n",
    "    bio=m_wakati.parse(str(bio)).strip()\n",
    "    bios.append(bio)\n",
    "\n",
    "trainings = [TaggedDocument(words = data.split(),tags = [i]) for i,data in enumerate(bios)]\n",
    "doc2vec = Doc2Vec(documents= trainings, dm = 1, vector_size=32, window=4, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bio = np.array([doc2vec.docvecs[i] for i in range(df_train.shape[0])])\n",
    "X_test_bio = np.array([doc2vec.infer_vector(m_wakati.parse(str(bio)).split(\" \")) for bio in df_test.bio])\n",
    "y_train = df_train[\"match\"].values\n",
    "y_test = df_test[\"match\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, model, n=4,**kwargs):\n",
    "    models = [copy.deepcopy(model) for i in range(n)]\n",
    "    y_preds = []\n",
    "    for i in range(n):\n",
    "        model = models[i]\n",
    "        slice1 = (X_train.shape[0]//n)*i\n",
    "        slice2 = (X_train.shape[0]//n)*(i+1)\n",
    "        \n",
    "        _X_train = np.concatenate([X_train[:slice1], X_train[slice2:]])\n",
    "        _y_train = np.concatenate([y_train[:slice1], y_train[slice2:]])\n",
    "        _X_test = X_train[slice1:slice2]\n",
    "        _y_test = y_train[slice1:slice2]\n",
    "        model.fit(_X_train, _y_train,**kwargs)\n",
    "        _y_pred = model.predict(_X_test)\n",
    "        print(roc_auc_score(_y_test, _y_pred))\n",
    "        y_preds.append(_y_pred)\n",
    "    y_pred = np.concatenate(y_preds)\n",
    "    return y_pred, models\n",
    "\n",
    "def predict(X_test, models):\n",
    "    y_pred = np.zeros(X_test.shape[0])\n",
    "    n = len(models)\n",
    "    for model in models:\n",
    "        y_pred += model.predict(X_test).reshape((-1,))/n\n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46947404754713723\n",
      "0.40435172014119386\n",
      "0.3798396802688648\n",
      "0.4435359181731684\n",
      "[19:31:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "0.5690555291884195\n",
      "[19:32:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "0.516648227174543\n",
      "[19:32:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "0.4831732406839703\n",
      "[19:32:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "0.43865960989533775\n"
     ]
    }
   ],
   "source": [
    "y_pred, models=train(X_train_bio, y_train, SVR(C=100))\n",
    "df_train[\"bio_svr\"] = y_pred\n",
    "df_test[\"bio_svr\"] = predict(X_test_bio, models)\n",
    "\n",
    "y_pred, models=train(X_train_bio, y_train, xgb.XGBRegressor(n_jobs=-1))\n",
    "df_train[\"bio_xgb\"] = y_pred\n",
    "df_test[\"bio_xgb\"] = predict(X_test_bio, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1170b40281b74fdfb496b0f3e1df64dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8472.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2260079f9a42aebd421ddc3b175969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def getImage(_id):\n",
    "    num_photos = int(df[df[\"id\"]==_id][\"photo_num\"])\n",
    "    imgs = []\n",
    "    for i in range(num_photos):\n",
    "        fileName = _id+\"-\"+str(i)+\".jpg\"\n",
    "        filePath = os.path.join(imagePath, fileName)\n",
    "        img = cv2.imread(filePath)\n",
    "        if img is None:\n",
    "            print(filePath)\n",
    "        img = cv2.resize(img, (120,120))\n",
    "        imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "#写真の読み込み\n",
    "X_train_images = []\n",
    "with Pool() as p:\n",
    "    imap=p.imap(getImage, df_train[\"id\"])\n",
    "    for photos in list(tqdm(imap, total=df_train.shape[0])):\n",
    "        for photo in photos:\n",
    "            X_train_images.append(photo)\n",
    "X_train_images = np.asarray(X_train_images)/255\n",
    "    \n",
    "y_train_image = []\n",
    "for photo_num, label in zip(df_train[\"photo_num\"], y_train):\n",
    "    y_train_image += [label]*photo_num\n",
    "y_train_image = np.array(y_train_image)\n",
    "\n",
    "\n",
    "X_test_images = []\n",
    "with Pool() as p:\n",
    "    imap=p.imap(getImage, df_test[\"id\"])\n",
    "    for photos in list(tqdm(imap, total=df_test.shape[0])):\n",
    "        for photo in photos:\n",
    "            X_test_images.append(photo)\n",
    "X_test_images = np.asarray(X_test_images)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15509/15509 [==============================] - 105s 7ms/step - loss: 0.0215 - mse: 0.0215\n",
      "Epoch 2/10\n",
      "15509/15509 [==============================] - 106s 7ms/step - loss: 0.0109 - mse: 0.0109\n",
      "Epoch 3/10\n",
      "15509/15509 [==============================] - 105s 7ms/step - loss: 0.0090 - mse: 0.0090\n",
      "Epoch 4/10\n",
      "15509/15509 [==============================] - 105s 7ms/step - loss: 0.0086 - mse: 0.0086\n",
      "Epoch 5/10\n",
      "15509/15509 [==============================] - 105s 7ms/step - loss: 0.0083 - mse: 0.0083\n",
      "Epoch 6/10\n",
      "15509/15509 [==============================] - 105s 7ms/step - loss: 0.0082 - mse: 0.0082\n",
      "Epoch 7/10\n",
      "15509/15509 [==============================] - 105s 7ms/step - loss: 0.0083 - mse: 0.0083\n",
      "Epoch 8/10\n",
      "15509/15509 [==============================] - 105s 7ms/step - loss: 0.0081 - mse: 0.0081\n",
      "Epoch 9/10\n",
      "15509/15509 [==============================] - 105s 7ms/step - loss: 0.0082 - mse: 0.0082\n",
      "Epoch 10/10\n",
      "15509/15509 [==============================] - 105s 7ms/step - loss: 0.0082 - mse: 0.0082\n",
      "0.5794453552671798\n",
      "Epoch 1/10\n",
      "15509/15509 [==============================] - 104s 7ms/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 2/10\n",
      "15509/15509 [==============================] - 104s 7ms/step - loss: 0.0108 - mse: 0.0108\n",
      "Epoch 3/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0089 - mse: 0.0089\n",
      "Epoch 4/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0082 - mse: 0.0082\n",
      "Epoch 5/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0080 - mse: 0.0080\n",
      "Epoch 6/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0080 - mse: 0.0080\n",
      "Epoch 7/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0080 - mse: 0.0080\n",
      "Epoch 8/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0080 - mse: 0.0080\n",
      "Epoch 9/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0079 - mse: 0.0079\n",
      "Epoch 10/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0081 - mse: 0.0081\n",
      "0.547918288884563\n",
      "Epoch 1/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0198 - mse: 0.0198\n",
      "Epoch 2/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0090 - mse: 0.0090\n",
      "Epoch 3/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0072 - mse: 0.0072\n",
      "Epoch 4/10\n",
      "15509/15509 [==============================] - 102s 7ms/step - loss: 0.0067 - mse: 0.0067\n",
      "Epoch 5/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0067 - mse: 0.0067\n",
      "Epoch 6/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0064 - mse: 0.0064\n",
      "Epoch 7/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0064 - mse: 0.0064\n",
      "Epoch 8/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0064 - mse: 0.0064\n",
      "Epoch 9/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0065 - mse: 0.0065\n",
      "Epoch 10/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0065 - mse: 0.0065\n",
      "0.5838020407437694\n",
      "Epoch 1/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0213 - mse: 0.0213\n",
      "Epoch 2/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0106 - mse: 0.0106\n",
      "Epoch 3/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0088 - mse: 0.0088\n",
      "Epoch 4/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0082 - mse: 0.0082\n",
      "Epoch 5/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0080 - mse: 0.0080\n",
      "Epoch 6/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0080 - mse: 0.0080\n",
      "Epoch 7/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0080 - mse: 0.0080\n",
      "Epoch 8/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0079 - mse: 0.0079\n",
      "Epoch 9/10\n",
      "15509/15509 [==============================] - 103s 7ms/step - loss: 0.0079 - mse: 0.0079\n",
      "Epoch 10/10\n",
      "15509/15509 [==============================] - 102s 7ms/step - loss: 0.0082 - mse: 0.0082\n",
      "0.5200201966128091\n"
     ]
    }
   ],
   "source": [
    "def getVGG16BasedModel():\n",
    "    model = VGG16(weights=\"imagenet\", include_top=False)\n",
    "    x = model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(1, activation=\"linear\")(x)\n",
    "    model = Model(inputs=model.input, outputs=predictions)\n",
    "    for layer in model.layers[:-3]:\n",
    "        layer.trainable=False\n",
    "    return model\n",
    "\n",
    "\n",
    "model = getVGG16BasedModel()\n",
    "model.compile(optimizer=Adam(), loss=\"mse\", metrics=[\"mse\"])\n",
    "y_pred, models = train(X_train_images, y_train_image, model, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "y_pred_photo = []\n",
    "for n in df_train[\"photo_num\"]:\n",
    "    y_pred_photo.append(np.mean(y_pred[s:s+n]))\n",
    "    s+=n\n",
    "y_pred_photo = np.array(y_pred_photo)\n",
    "df_train[\"photo\"] = y_pred_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94fbf3bcdc64ae3aee52d2b6ea2cd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_images = []\n",
    "with Pool() as p:\n",
    "    imap=p.imap(getImage, df_test[\"id\"])\n",
    "    for photos in list(tqdm(imap, total=df_test.shape[0])):\n",
    "        for photo in photos:\n",
    "            X_test_images.append(photo)\n",
    "X_test_images = np.asarray(X_test_images)/255\n",
    "    \n",
    "y_pred = predict(X_test_images, models)\n",
    "\n",
    "s = 0\n",
    "y_pred_photo = []\n",
    "for n in df_test[\"photo_num\"]:\n",
    "    y_pred_photo.append(np.mean(y_pred[s:s+n]))\n",
    "    s+=n\n",
    "y_pred_photo = np.array(y_pred_photo)\n",
    "df_test[\"photo\"] = y_pred_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df_train[[\"age\",\"distance_mi\",\"photo_num\",\"bio_svr\",\"bio_xgb\",\"photo\"]]\n",
    "df2_test=df_test[[\"age\",\"distance_mi\",\"photo_num\",\"bio_svr\",\"bio_xgb\",\"photo\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"photo\"]=df2[\"photo\"].fillna(df2[\"photo\"].mean())\n",
    "df2[\"distance_mi\"][df2[\"distance_mi\"]==\"undefined\"]=np.mean(df2[\"distance_mi\"][df2[\"distance_mi\"]!=\"undefined\"])\n",
    "df2[\"age\"][df2[\"age\"]==\"undefined\"]=np.mean(df2[\"age\"][df2[\"age\"]!=\"undefined\"])\n",
    "df2=df2.astype(np.float32)\n",
    "df2_test[\"photo\"]=df2_test[\"photo\"].fillna(df2[\"photo\"].mean())\n",
    "df2_test[\"distance_mi\"][df2_test[\"distance_mi\"]==\"undefined\"]=np.mean(df2[\"distance_mi\"][df2[\"distance_mi\"]!=\"undefined\"])\n",
    "df2_test[\"age\"][df2_test[\"age\"]==\"undefined\"]=np.mean(df2[\"age\"][df2[\"age\"]!=\"undefined\"])\n",
    "df2_test=df2_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:53:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:54:23] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:54:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:54:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:55:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:55:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[20:56:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42,n_jobs=-1)),\n",
    "    ('svr', make_pipeline(StandardScaler(),\n",
    "                          SVR())),\n",
    "    (\"xgb\",xgb.XGBRegressor(n_jobs=-1)),\n",
    "    (\"ada\",AdaBoostRegressor()),\n",
    "    (\"bag\",BaggingRegressor()),\n",
    "    (\"gauss\",GaussianProcessRegressor()),\n",
    "    (\"Sgd\",SGDRegressor()),\n",
    "    (\"Huber\",HuberRegressor()),\n",
    "    (\"TheilSen\",TheilSenRegressor())\n",
    "]\n",
    "clf = StackingRegressor(\n",
    "    estimators=estimators, final_estimator=xgb.XGBRegressor()\n",
    ")\n",
    "\n",
    "y_pred=clf.fit(df2, y_train).predict(df2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6672531304485656"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
